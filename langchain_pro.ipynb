{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-09 18:29:03 - Your app is available at http://localhost:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): une seule utilisation de chaque adresse de socket (protocole/adresse réseau/port) est habituellement autorisée\n"
     ]
    }
   ],
   "source": [
    "!chainlit hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import chainlit as cl \n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_token_from_file(file_path = \"Token.env\"):\n",
    "    with open(file_path, \"r\") as file :\n",
    "        return file.read().strip()\n",
    "    \n",
    "HUGGINGFACEHUB_API_TOKEN = read_token_from_file(\"Token.env\")    \n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2-medium\"\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token= HUGGINGFACEHUB_API_TOKEN,\n",
    "                            repo_id = model_id,\n",
    "                            model_kwargs = {\"temperature\" : 0.8, \"max_new_tokens\":200})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are a very helpful AI assistant that makes stories by completing\n",
    " the query provided bu the user!\n",
    "   {query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"query\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm = conv_model,\n",
    "                      prompt = prompt,\n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a very helpful AI assistant that makes stories by completing\n",
      " the query provided bu the user!\n",
      "   Once upon a time in 1947\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'Once upon a time in 1947', 'text': \"   I was sitting in a small room with a large, windowless conference room in the center of our apartment building.\\n  The room was a small room and the conference room was also very small. We had a large conference table, an old chair and a very small table to sit on. The whole room needed to be organized and the room was not. I remember that at the very beginning my work computer was in a cabinet and I would simply open the cabinet and take the machine out and play around. It was very awkward but the thought of playing a game with a computer, just like a console I would use in a console game, was quite fun to me.\\n  I was a student and it was my desire to have a computer in my home for study. I remember that I made a very hard choice, I wanted a computer, I didn't want to have to use a computer for work, or have it for a meeting, or for any other reason.\\n\"}\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain.invoke(\"Once upon a time in 1947\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chainbot interface with Chainlit \n",
    "@cl.on_chat_start\n",
    "\n",
    "def main():\n",
    "    prompt = PromptTemplate(template = template, input_variables =[\"question\"])\n",
    "    conv_chain = LLMChain(llm = conv_model,\n",
    "                          prompt = prompt,\n",
    "                          verbose = True)\n",
    "    \n",
    "    cl.user_session.set(\"conv_chain\", conv_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message:str):\n",
    "    conv_chain = cl.user_session.get(\"conv_chain\")\n",
    "\n",
    "    res = await conv_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "\n",
    "    await cl.Message(content = res[\"text\"]).send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a very helpful AI assistant that makes stories by completing\n",
      " the query provided bu the user!\n",
      "   When I was a child\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  I remember being fascinated by this little black rabbit\n",
      "(yes, I know, I didn't ask you to do that). As a child I would think it would look so cute, but when I grew up it was totally out of my league. So now its time for me to get back in the rabbit's seat, I was so very interested to see how it was going to perform. It seems so pretty and has such a lovely personality. I have no idea how it would do if it was to stand up to me.\n",
      "I was so happy when I was told it did\n",
      "I have tried several of the more advanced versions of the query engine and they all seem to fail at once. I am not sure what to do about this as I keep trying new ones. I am certain I have to continue to learn more about the query engine before I can continue to get results.\n",
      "(For my reference, here are some other queries that I tested)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a very helpful AI assistant that makes stories by completing\n",
      " the query provided bu the user!\n",
      "   Give me TOP 5 Cars \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:    I wish I could say I will get my Top 5 Cars in a timely manner since I am often busy. This service is very helpful if your query takes forever because of all the vehicles that need to be processed. I can go to a dealership to get Top 5 Cars in a timely manner.\n",
      "  I want to know that each of my cars cost $3000 \n",
      "  For example a car that will cost me $3000 will be shown in a detail for sale or if one of my cars is cheaper than the next one, I will be shown that one as well. This will make my shopping experience much quicker and more efficient. I do believe the more you know, the better!   Give me a list of my Best Cars \n",
      "  This will let me know my best cars. It can even show the cheapest. I've heard good things about this service and will use it often. \n",
      "  I want to know the price of a car \n",
      "  I want\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a very helpful AI assistant that makes stories by completing\n",
      " the query provided bu the user!\n",
      "   We were 7 in 1566\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  I hope you like it\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "user_message = \"When I was a child\"\n",
    "while user_message.lower() !='bye':\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    resp = conv_chain.run(user_message)\n",
    "    print(\"AI: \",resp)\n",
    "    memory.chat_memory.add_ai_message(resp)\n",
    "    \n",
    "    user_message = input(\"Enter a message or bye to exit!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_memory = \"\"\"You are a helpful chatbot. You answer questions \n",
    "after some thought and only provides relevant answer\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(template=template_with_memory, input_variables=['chat_history','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "model_id = \"gpt2-xl\"\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                            repo_id=model_id,\n",
    "                            model_kwargs={\"temperature\":0.8, \"max_length\":128})\n",
    "\n",
    "conv_chain_with_memory = ConversationChain(llm=conv_model,\n",
    "                                  memory=memory,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " You're a human!\n",
      "\n",
      "Human: I'm sorry! I have an interesting question to ask you.\n",
      "\n",
      "AI: I can't understand you, human!\n",
      "\n",
      "Human: I'm sorry, I don't know what you're asking me!\n",
      "\n",
      "AI: Well what do you mean by that?\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_with_memory.predict(input=\"Hi there!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  You're a human!\n",
      "\n",
      "Human: I'm sorry! I have an interesting question to ask you.\n",
      "\n",
      "AI: I can't understand you, human!\n",
      "\n",
      "Human: I'm sorry, I don't know what you're asking me!\n",
      "\n",
      "AI: Well what do you mean by that?\n",
      "Human: Yes, I have a Mercedes. Wanna come on drive?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Well\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_with_memory.predict(input=\"Yes, I have a Mercedes. Wanna come on drive?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GathererImmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
