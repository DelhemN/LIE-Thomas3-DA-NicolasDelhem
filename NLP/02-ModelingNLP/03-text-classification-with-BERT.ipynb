{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS4troYKCHXW"
      },
      "source": [
        "# Text Classification with BERT\n",
        "\n",
        "![bert](https://res.cloudinary.com/practicaldev/image/fetch/s--ozy733MJ--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/q5e65ugnue96bir3usyk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5lJkWQCKqE"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a NLP model developed by Google in 2018. It is a model that is already pre-trained on a 2,5000M (+- 170 GB) words corpus from Wikipedia.\n",
        "\n",
        "To accomplish a particular NLP task, the pre-trained BERT model is used as a base and refined by adding an additional layer; the model can then be trained on a labeled dataset dedicated to the NLP task to be performed. This is the very principle of transfer learning. It is important to note that BERT is a very large model with 12 layers, 12 attention heads and 110 million parameters (BERT base).\n",
        "\n",
        "The BERT model is able to do :\n",
        "\n",
        "*   Translation\n",
        "*   Text generation\n",
        "*   Classification\n",
        "*   Question-answering\n",
        "*   ...\n",
        "\n",
        "### Why BERT?\n",
        "\n",
        "Using the General Language Understanding Evaluation ([GLUE](https://gluebenchmark.com/)) benchmark [leaderboard](https://gluebenchmark.com/leaderboard) , its easy to realize that many models on the list are all forks of BERT.\n",
        "\n",
        "## Let's go !\n",
        "To use BERT you need to have either pytorch or tensorflow installed in your environment. It is also preferable to have access to a GPU on your computer. If you don't have a GPU you can use [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "Next, let’s install the transformers package from Hugging Face. This package is an interface between BERT and pytorch and/or tensorflow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_111Kiv1_x7A",
        "outputId": "0b164f54-e57e-460f-96b1-758c4ebb48f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicol\\anaconda3\\envs\\gathererimmo\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IykzH85CDoUl"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "For this project we will use the data from Odile. Odile is a bot that tries to answer general questions on a few BeCode Discord servers. The sentences all come from conversations between learners and Odile on Discord.\n",
        "\n",
        "You'll find the data in `./dataset/odile_data.csv`. You can import them in a dataframe and display it.\n",
        "\n",
        "**Tip:** if you are using Google colab you can import the CSV in your google drive and connect your notebook to your Google drive (check on Google how to do that !)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "gJVn61y9_2Rm",
        "outputId": "d18a8a98-ce34-43fa-a66a-7b8fee9962b6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv(\"./dataset/odile_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1keVkxwE_R5"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "It's time to take a quick look at our data.\n",
        "\n",
        "As you see the questions from the learners are classified as intents (i.e. the goal the user has in mind when typing in a question or comment)\n",
        "\n",
        "**Exercise:** Use your data exploration and visualization skills to answer the the following questions:\n",
        "\n",
        "*   How many observations does the dataset contain?\n",
        "*   How many different labels does the dataset contain?\n",
        "*   Which labels contain the most observations?\n",
        "*   Which labels contain the fewest observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "twAXoP2yFSKi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1555 Index(['sentence', 'intent'], dtype='object')\n",
            "1543\n",
            "bye\n",
            "see you\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#How many observations does the dataset contain?\n",
        "print(df.shape[0], df.columns)\n",
        "#  How many different labels does the dataset contain?\n",
        "num_labels = df[\"sentence\"].nunique()\n",
        "print(num_labels)\n",
        "#Which labels contain the most observations?\n",
        "most_common_label = df[\"sentence\"].value_counts().idxmax()\n",
        "print(most_common_label)\n",
        "#Which labels contain the fewest observations?\n",
        "least_common_label = df[\"sentence\"].value_counts().idxmin()\n",
        "print(least_common_label)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeVa52nFSpg"
      },
      "source": [
        "## It's time to clean up !\n",
        "\n",
        "\n",
        "Not all NLP tasks require the same preprocessing. In this case, we have to ask ourselves some questions: \n",
        "\n",
        "- Are there unwanted characters in the dataset? For example, do you want to keep the smiley's or not?  \n",
        "  - If, for example, you want to create labels to analyze feelings, it might be perishable to keep the smiley's.\n",
        "- Is it relevant to keep capital letters in sentences?\n",
        "  - In this case, capital letters don't really matter, because on one hand, not everyone starts their sentences with capital letters when chatting. On the other hand, the sentences are quite short, addressed directly to Odile. \n",
        "- Is it necessary to limit the number of characters in a sentence?\n",
        "  - Again in this case it may be preferable to limit the number of words. The questions asked to Odile are supposed to be short, as too long sentences could interfere with the classification if they contain too much information.\n",
        "\n",
        "There is no universal answer. Everything will depend on the expected result. \n",
        "\n",
        "**Exercise :** Clean the dataset.\n",
        "- Remove all unnecessary characters. You can choose to keep the smiley's or not.\n",
        "- Put all sentences in lower case.\n",
        "- Limit text to 256 words.\n",
        "\n",
        "What other preprocessing steps can you think of?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "Z8ZVkTmDGCVe",
        "outputId": "9dfcadfa-23ee-416b-9f9d-dbebbde195d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>intent</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>who are you?</td>\n",
              "      <td>smalltalk_agent_acquaintance</td>\n",
              "      <td>who are you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>all about you</td>\n",
              "      <td>smalltalk_agent_acquaintance</td>\n",
              "      <td>all about you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what is your personality</td>\n",
              "      <td>smalltalk_agent_acquaintance</td>\n",
              "      <td>what is your personality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>define yourself</td>\n",
              "      <td>smalltalk_agent_acquaintance</td>\n",
              "      <td>define yourself</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what are you</td>\n",
              "      <td>smalltalk_agent_acquaintance</td>\n",
              "      <td>what are you</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   sentence                        intent  \\\n",
              "0              who are you?  smalltalk_agent_acquaintance   \n",
              "1             all about you  smalltalk_agent_acquaintance   \n",
              "2  what is your personality  smalltalk_agent_acquaintance   \n",
              "3           define yourself  smalltalk_agent_acquaintance   \n",
              "4              what are you  smalltalk_agent_acquaintance   \n",
              "\n",
              "             clean_sentence  \n",
              "0               who are you  \n",
              "1             all about you  \n",
              "2  what is your personality  \n",
              "3           define yourself  \n",
              "4              what are you  "
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-zA-Z\\'\\-]\", \" \", text)\n",
        "    return \" \".join(word_tokenize(text)[:256])\n",
        "\n",
        "df[\"clean_sentence\"] = df.sentence.apply(clean_text)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLc9G7nsGNir"
      },
      "source": [
        "## Defining observations (`X`) and labels (`y`)\n",
        "\n",
        "As you know, training a model requires a set of observations (`X`) and their corresponding labels (`y`).\n",
        "\n",
        "In that case, `X` is your clean text and `y` is the intent.\n",
        "\n",
        "Do not forget that we are dealing with a multi-class classification problem. Then, you may have to **one-hot encode** the target value. Keep track of the mapping between the one-hot encoding and the labels in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "T2Cce6LYG8hU"
      },
      "outputs": [],
      "source": [
        "X = df[\"clean_sentence\"].to_list()\n",
        "y = pd.get_dummies(df[\"intent\"])\n",
        "\n",
        "mapping = {i:name for i,name in enumerate (y.columns) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirU5x74IJrg"
      },
      "source": [
        "## Split your dataset!\n",
        "\n",
        "After all this time, I dare to hope that it is not necessary to explain this step anymore!\n",
        "\n",
        "**Exercise :** Create the variables `X_train`, `X_val`, `X_test`, `y_train`, `y_val` and `y_test`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Dxit-YG0ou56"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split Train and Validation data\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Keep some data for inference (testing)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m80bg5t0Jzq9"
      },
      "source": [
        "## Tokenization \n",
        "If you don't know what tokenization is anymore: look [here](../1.preprocessing/1.tokenization.ipynb).\n",
        "\n",
        "We will use the tokenizer provided by BERT. This is a pre-trained model that will save us time. \n",
        "\n",
        "**Exercise :** Create a `tokenizer` variable and instantiate `DistilBertTokenizer.from_pretrained()` from `transformers`. You have to load `distilbert-base-uncased` model. (Uncased for case-insensitive.) \n",
        "\n",
        "Read more: [Tokenizer documentation](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHma6LtqfLs",
        "outputId": "bf329bf5-40d2-41f8-f290-5b6bc9317881"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cQ19esLKBIK"
      },
      "source": [
        "### Tokenize the dataset\n",
        "\n",
        "Good! We have instantiated our tokenizer but we have not yet encoded our words in vector.\n",
        "To do this we will have to apply the tokenizer on our dataset. This will convert our texts into vectors.\n",
        "\n",
        "\n",
        "**Exercise:** Create the `train_encodings`, `val_encodings` and `test_encodings` by calling the tokenizer on `X_train`,  `X_val` and `X_test`.\n",
        "\n",
        "You need to know 3 parameters. \n",
        "\n",
        "- **max_length:** Maximum length of the sequence. You can set it to 200\n",
        "- **truncation:** This will truncate to a maximum length specified by the max_length argument. This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached. You can set it to `True`\n",
        "- **padding:** this is the parameter to make all vectors have the same length. You can set it to `True`.\n",
        "\n",
        "[More info here](https://huggingface.co/docs/transformers/preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "xITb1MwhKH8H"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(X_train, max_length=200, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(X_val, max_length=200, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test, max_length=200, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETZODcPViDKH"
      },
      "source": [
        "## Prepare the datasets for training\n",
        "\n",
        "You can now convert your training, evaluation and test sets in a dataset that will contain both observations and labels. Use the [from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) method from Tensorflow to create the datasets. This methods takes two arguments:\n",
        "\n",
        "*   The encodings that you have just created (casted as a `dict`)\n",
        "*   The labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Jyaj7R9ppBc7"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    y_val\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaazZtg3j0jE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--J4LcUTkHjQ"
      },
      "source": [
        "### Load BERT model\n",
        "\n",
        "You will need to load the BERT pre-trained model by using the class `TFDistilBertForSequenceClassification`\n",
        "\n",
        "⚠️ You must use the same model as the one used for tokenization. So in our case  `distilbert-base-uncased`. \n",
        "\n",
        "* [BERT for Sequence Classification Documentation](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n",
        "\n",
        "**Exercise:** Create a model variable and load it by using  `TFDistilBertForSequenceClassification.from_pretrained()` As a parameter, you must indicate the number of labels (get this number from your original dataframe).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXIMee4HpNM-",
        "outputId": "4e6950d6-72ac-4872-fec2-1939ac59ea1f"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "\nTFDistilBertForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[88], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertForSequenceClassification\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFDistilBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(y)))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1288\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1288\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1271\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1273\u001b[0m checks \u001b[38;5;241m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends)\n\u001b[0;32m   1274\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
            "\u001b[1;31mImportError\u001b[0m: \nTFDistilBertForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_label = len(set(y)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnPAcyWmykz"
      },
      "source": [
        "### Training arguments\n",
        "\n",
        "Let's define the the training arguments and compile our model\n",
        "\n",
        "*   Define the optimizer (Adam) and its learning rate\n",
        "*   Define the loss function that will be used (remember that we have one-hot encoded output data)\n",
        "*   Define the evaluation appropriate metrics\n",
        "*   Compile the model with the right metrics\n",
        "*   Display the model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5VYs6n6mpnZ",
        "outputId": "ecb07717-82eb-45ed-9530-4338901a0d36"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER =  \n",
        "LOSS = \n",
        "METRICS = \n",
        "\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGqIC2D12hjx"
      },
      "source": [
        "### Training\n",
        "\n",
        "Define first the number of epochs and the batch size for the training.\n",
        "\n",
        "The batch size will depend on your machine. If you have a weak GPU, I advise you to put 8 or 16.\n",
        "\n",
        "The number of epochs will depend on your machine, the batch size, etc...You can start with 5 for example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulixSczoHI0P"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E96Ad6lcpYdB",
        "outputId": "df318820-ff12-400e-b8c1-bee84123b49b"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset.batch(BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRfXFwZHInj4"
      },
      "source": [
        "### Plot the learning curve of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "vyh-V9Db36op",
        "outputId": "956ac673-1d15-47f1-ae70-50d27e859e10"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
        "    that is output from your `fit` method to plot the loss and accuracy of\n",
        "    the training and validation set.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
        "    axs[0].plot(history.history['accuracy'], label='training set')\n",
        "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
        "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, 1])\n",
        "\n",
        "    axs[1].plot(history.history['loss'], label='training set')\n",
        "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
        "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 10])\n",
        "    \n",
        "    axs[0].legend(loc='lower right')\n",
        "    axs[1].legend(loc='lower right')\n",
        "    \n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR0RWyeLIvQo"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "We can now evaluate our model on the test set. Use the `model.evaluate()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvHTZCujIyVt",
        "outputId": "b703e285-4b61-4633-b6f6-3402b80c861f"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset.batch(BATCH_SIZE))\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFB1j6bxa04"
      },
      "source": [
        "**Exercise:** is the accuracy the best metrics for this dataset ? Explain your answer !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AJ0X6FgJg2B"
      },
      "source": [
        "## Test your model\n",
        "\n",
        "Well done, you did it :-)\n",
        "\n",
        "Oh...I have an idea ! Try to classify the sentence *Well done !* with your model\n",
        "\n",
        "Think to apply all the preprocessing steps and predict the intent of the user.\n",
        "\n",
        "**Tip:** use the mapping you have created above to retrieve the original label of the prediction !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLdTyxKnBCej",
        "outputId": "f8b15540-52dd-4f83-acbf-318e36247e06"
      },
      "outputs": [],
      "source": [
        "text = \"Well done !\"\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1d59efe0a6ac6ca00d64bcda3954588e00cc9223d62022edd1b9de1af7efdfc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
