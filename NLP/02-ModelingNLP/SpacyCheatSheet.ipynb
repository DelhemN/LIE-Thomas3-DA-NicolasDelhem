{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CheatSheet from BlackBox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "a\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('Apple is looking at buying a U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#Find occurences of a term in a document\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN NNP nsubj Xxxxx True False\n",
      "is AUX VBZ aux xx True True\n",
      "looking VERB VBG ROOT xxxx True False\n",
      "at ADP IN prep xx True True\n",
      "buying VERB VBG pcomp xxxx True False\n",
      "a DET DT det x True True\n",
      "U.K. PROPN NNP compound X.X. False False\n",
      "startup NOUN NN dobj xxxx True False\n",
      "for ADP IN prep xxx True True\n",
      "$ SYM $ quantmod $ False False\n",
      "1 NUM CD compound d False False\n",
      "billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "#Understating POS Tagging\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_,token.dep_,token.shape_,token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 29 33 GPE\n",
      "$1 billion 46 56 MONEY\n"
     ]
    }
   ],
   "source": [
    "#Extracting Named Entities \n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple nsubj looking VERB\n",
      "is aux looking VERB\n",
      "looking ROOT looking VERB\n",
      "at prep looking VERB\n",
      "buying pcomp at ADP\n",
      "a det startup NOUN\n",
      "U.K. compound startup NOUN\n",
      "startup dobj buying VERB\n",
      "for prep startup NOUN\n",
      "$ quantmod billion NUM\n",
      "1 compound billion NUM\n",
      "billion pobj for ADP\n"
     ]
    }
   ],
   "source": [
    "#Navigating and Searching Semantic Dependencies:\n",
    "\n",
    "for token in doc: \n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.649502\n",
      "0.03433087095618248\n",
      "6418411030699964375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_33048\\626990040.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc[0].similarity(doc[1]))\n"
     ]
    }
   ],
   "source": [
    "#Word Vector Representations\n",
    "\n",
    "print(doc[0].vector_norm)\n",
    "print(doc[0].similarity(doc[1]))\n",
    "print(doc[0].orth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "a U.K. startup\n",
      "Subject : Apple\n",
      "Object :  startup\n",
      "[('Apple', 'NNP', 'looking', 'nsubj'), ('is', 'VBZ', 'looking', 'aux'), ('looking', 'VBG', 'looking', 'ROOT'), ('at', 'IN', 'looking', 'prep'), ('buying', 'VBG', 'at', 'pcomp'), ('a', 'DT', 'startup', 'det'), ('U.K.', 'NNP', 'startup', 'compound'), ('startup', 'NN', 'buying', 'dobj'), ('for', 'IN', 'startup', 'prep'), ('$', '$', 'billion', 'quantmod'), ('1', 'CD', 'billion', 'compound'), ('billion', 'CD', 'for', 'pobj')]\n"
     ]
    }
   ],
   "source": [
    "#Parsing and Extracting Information\n",
    "\n",
    "#Noun phrases\n",
    "for np in doc.noun_chunks:\n",
    "    print(np.text)\n",
    "\n",
    "\n",
    "#Subject, Verb, Object(SVO) triples\n",
    "for token in doc:\n",
    "    if token.dep_ =='nsubj':\n",
    "        print(\"Subject :\", token.text)\n",
    "    elif token.dep_ == 'dobj':\n",
    "        print(\"Object : \", token.text)\n",
    "\n",
    "#Token attributes\n",
    "\n",
    "print([(token.text, token.tag_,token.head.text, token.dep_) for token in doc])                        \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple looking nsubj <generator object at 0x000001B74E961AB0>\n",
      "is looking aux <generator object at 0x000001B74E961AB0>\n",
      "looking looking ROOT <generator object at 0x000001B74E961AB0>\n",
      "at looking prep <generator object at 0x000001B74E961AB0>\n",
      "buying at pcomp <generator object at 0x000001B74E961AB0>\n",
      "a startup det <generator object at 0x000001B74E961AB0>\n",
      "U.K. startup compound <generator object at 0x000001B74E961AB0>\n",
      "startup buying dobj <generator object at 0x000001B74E961AB0>\n",
      "for startup prep <generator object at 0x000001B74E961AB0>\n",
      "$ billion quantmod <generator object at 0x000001B74E961AB0>\n",
      "1 billion compound <generator object at 0x000001B74E961AB0>\n",
      "billion for pobj <generator object at 0x000001B74E961AB0>\n"
     ]
    }
   ],
   "source": [
    "#Syntactic Parsing\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.head.text, token.dep_, token.children)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GathererImmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
