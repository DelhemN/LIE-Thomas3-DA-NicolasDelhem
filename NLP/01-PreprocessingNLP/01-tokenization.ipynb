{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "To understand a sentence, us as humans, read each word. We analyze the meaning of it and then we connect words together. It's the same for a machine. So the first step to almost any NLP task will be to split sentences in words.\n",
    "\n",
    "![tokens](https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg)\n",
    "\n",
    "It seems simple said like that, but you also have to slice punctuation, compound words (but not all of them),...\n",
    "\n",
    "It's a time consuming task, that's why people invented a \"Tokenization\" function.\n",
    "\n",
    "Let's have a look at how [Spacy](https://spacy.io/) handles that.\n",
    "\n",
    "## Installation\n",
    "\n",
    "You will need to install Spacy, to do that I let you search on [their website](https://spacy.io/).\n",
    "You will also need to download their English model `en_core_web_sm`. To do that you can type:\n",
    "```shell\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "You can see that Spacy provides us with models in a lot of languages. We won't use them now but keep that in mind. It can be useful for later !\n",
    "\n",
    "## Tokenize the text\n",
    "\n",
    "Now that you installed Spacy, let's take a look at their basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Tokenize this document with SpaCy:\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "\n",
    "\n",
    "import spacy \n",
    "\n",
    "#Load the model \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, our text is tokenized, now we can see a lot of interesting features. But first of all, let's see what our tokens look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "was\n",
      "n’t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this:\n",
    "```\n",
    "When\n",
    "Sebastian\n",
    "Thrun\n",
    "started\n",
    "working\n",
    "on\n",
    "self\n",
    "-\n",
    "driving\n",
    "cars\n",
    "at\n",
    "Google\n",
    "in\n",
    "2007\n",
    ",\n",
    "few\n",
    "people\n",
    "outside\n",
    "of\n",
    "the\n",
    "company\n",
    "took\n",
    "him\n",
    "seriously\n",
    ".\n",
    "“\n",
    "I\n",
    "can\n",
    "tell\n",
    "you\n",
    "very\n",
    "senior\n",
    "CEOs\n",
    "of\n",
    "major\n",
    "American\n",
    "car\n",
    "companies\n",
    "would\n",
    "shake\n",
    "my\n",
    "hand\n",
    "and\n",
    "turn\n",
    "away\n",
    "because\n",
    "I\n",
    "was\n",
    "n’t\n",
    "worth\n",
    "talking\n",
    "to\n",
    ",\n",
    "”\n",
    "said\n",
    "Thrun\n",
    ",\n",
    "in\n",
    "an\n",
    "interview\n",
    "with\n",
    "Recode\n",
    "earlier\n",
    "this\n",
    "week\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the punctuation and `-` have been separated from the word they were appended to.\n",
    "\n",
    "Spacy also applies a lot of other preprocessing steps that we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package spacy:\n",
      "\n",
      "NAME\n",
      "    spacy\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    about\n",
      "    attrs\n",
      "    cli (package)\n",
      "    compat\n",
      "    displacy (package)\n",
      "    errors\n",
      "    git_info\n",
      "    glossary\n",
      "    kb (package)\n",
      "    lang (package)\n",
      "    language\n",
      "    lexeme\n",
      "    lookups\n",
      "    matcher (package)\n",
      "    ml (package)\n",
      "    morphology\n",
      "    parts_of_speech\n",
      "    pipe_analysis\n",
      "    pipeline (package)\n",
      "    schemas\n",
      "    scorer\n",
      "    strings\n",
      "    symbols\n",
      "    tests (package)\n",
      "    tokenizer\n",
      "    tokens (package)\n",
      "    training (package)\n",
      "    ty\n",
      "    util\n",
      "    vectors\n",
      "    vocab\n",
      "\n",
      "FUNCTIONS\n",
      "    blank(name: str, *, vocab: Union[spacy.vocab.Vocab, bool] = True, config: Union[Dict[str, Any], confection.Config] = {}, meta: Dict[str, Any] = {}) -> spacy.language.Language\n",
      "        Create a blank nlp object for a given language code.\n",
      "        \n",
      "        name (str): The language code, e.g. \"en\".\n",
      "        vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      "        config (Dict[str, Any] / Config): Optional config overrides.\n",
      "        meta (Dict[str, Any]): Overrides for nlp.meta.\n",
      "        RETURNS (Language): The nlp object.\n",
      "    \n",
      "    load(name: Union[str, pathlib.Path], *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Union[str, Iterable[str]] = [], enable: Union[str, Iterable[str]] = [], exclude: Union[str, Iterable[str]] = [], config: Union[Dict[str, Any], confection.Config] = {}) -> spacy.language.Language\n",
      "        Load a spaCy model from an installed package or a local path.\n",
      "        \n",
      "        name (str): Package name or model path.\n",
      "        vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      "        disable (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to disable. Disabled\n",
      "            pipes will be loaded but they won't be run unless you explicitly\n",
      "            enable them by calling nlp.enable_pipe.\n",
      "        enable (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to enable. All other\n",
      "            pipes will be disabled (but can be enabled later using nlp.enable_pipe).\n",
      "        exclude (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to exclude. Excluded\n",
      "            components won't be loaded.\n",
      "        config (Dict[str, Any] / Config): Config overrides as nested dict or dict\n",
      "            keyed by section values in dot notation.\n",
      "        RETURNS (Language): The loaded nlp object.\n",
      "\n",
      "DATA\n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterable = typing.Iterable\n",
      "        A generic version of collections.abc.Iterable.\n",
      "    \n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        On Python 3.10 and higher, the | operator\n",
      "        can also be used to denote unions;\n",
      "        X | Y means the same thing to the type checker as Union[X, Y].\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str]. Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            assert Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            assert Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            assert Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            assert Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "    \n",
      "    logger = <Logger spacy (WARNING)>\n",
      "\n",
      "VERSION\n",
      "    3.7.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\nicol\\appdata\\roaming\\python\\python311\\site-packages\\spacy\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spacy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GathererImmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
